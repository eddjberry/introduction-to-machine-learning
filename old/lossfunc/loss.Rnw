% !Rnw root = ../mlcrash.Rnw

<<losssetup, cache=FALSE, include=FALSE>>=
opts_knit$set(self.contained=FALSE)
@



%%%%%%%%%%%%%%%%%%%%%%%%
\part{The Loss Function}
%%%%%%%%%%%%%%%%%%%%%%%%
\newthought{Given a set of predictor variables} $X$ and some response $y$, we look for some function $f(X)$ to make predictions of y from those input variables.  We also need a function to penalize errors in prediction- a \emph{loss function}, $L(Y, f(X))$.  With chosen loss function, we then find the model which will minimize loss, generally speaking.  We will start with the familiar and note a couple others that might be used.

\section{Continuous Outcomes}
\subsection{Squared Error}
The classic loss function for linear models with continuous response is the squared error loss function, or the residual sum of squares.

\vspace{.25cm}
\noindent$L(Y, f(X)) = \sum(y-f(X))^2$
\vspace{.25cm}

\subsection{Absolute Error}
For an approach more robust to extreme observations, we might choose absolute rather than squared error as follows.  In this case, predictions are a conditional median rather than a conditional mean.

\vspace{.25cm}
\noindent$L(Y, f(X)) = \sum|(y-f(X))|$
\vspace{.25cm}

\subsection{Negative Log-likelihood}
We can also think of our usual likelihood methods learned in a standard applied statistics course as incorporating a loss function that is the negative log-likelihood pertaining to the model of interest.  If we assume a normal distribution for the response we can note the loss function as:

\vspace{.25cm}
\noindent$L(Y, f(X)) = n\ln{\sigma} + \sum \frac{1}{2\sigma^2}(y-f(X))^2$
\vspace{.25cm}

In this case it would converge to the same answer as the squared error/least squares solution.


\subsection{R Example}
The following provides code that one could use with the \textcolor{red}{optim} function in R to find estimates of regression coefficients (beta) that minimize the squared error.  X is a design matrix of our predictor variables with the first column a vector of 1s in order to estimate the intercept.  y is the continuous variable to be modeled\sidenote{Type ?optim at the console for more detail.}.

<<squareloss, cache=TRUE, warning=F, eval=TRUE, tidy=TRUE, size="footnotesize">>=
sqerrloss = function(beta, X, y){
  mu = X%*%beta
  sum((y-mu)^2)
}

set.seed(123)
X = cbind(1, rnorm(100), rnorm(100))
y = rowSums(X[,-1] + rnorm(100))
out1 = optim(par=c(0,0,0), fn=sqerrloss, X=X, y=y)
out2 = lm(y ~ X[,2] + X[,3])  # check with lm 
rbind(c(out1$par, out1$value), c(coef(out2),sum(resid(out2)^2)) )
@

\section{Categorical Outcomes}
Here we'll also look at some loss functions useful in classification problems. Note that there is not necessary exclusion in loss functions for continuous vs. categorical outcomes\sidenote{For example, we could use minimize squared errors in the case of classification also.}.

\subsection{Misclassification}
Probably the most straightforward is misclassification, or 0-1 loss.  If we note $f$ as the prediction, and for convenience we assume a [-1,1] response instead of a [0,1] response:

\vspace{.25cm}
\noindent$L(Y, f(X)) = \sum I(y\neq sign(f))$
\vspace{.25cm}

In the above, \emph{I} is the indicator function and so we are summing misclassifications.


\subsection{Binomial log-likelihood}

\vspace{.25cm}
\noindent$L(Y, f(X)) = \sum log(1 + e^{-2yf})$
\vspace{.25cm}

The above is in deviance form, but is equivalent to binomial log likelihood if $y$ is on the 0-1 scale.

\subsection{Exponential}
Exponential loss is yet another loss function at our disposal.

\vspace{.25cm}
\noindent$L(Y, f(X)) = \sum e^{-yf}$
\vspace{.25cm}

\subsection{Hinge Loss}
A final loss function to consider, typically used with support vector machines, is the hinge loss function.

\vspace{.25cm}
\noindent$L(Y, f(X)) = \max(1-yf, 0)$
\vspace{.25cm}

Here negative values of $yf$ are misclassifications, and so correct classifications do not contribute to the loss.  We could also note it as $\sum (1-yf)_+$ , i.e. summing only those positive values of $1-yf$.

\vspace{.25cm}
\marginnote{\includegraphics[scale=.5]{images/lossfuncs}}
Which of these might work best may be specific to the situation, but the gist is that they penalize negative values (misclassifications) more heavily and increasingly so the worse the misclassification (except for misclassification error, which penalizes all misclassifications equally), with their primary difference in how heavy that penalty is.  At right is a depiction of the loss as a functions above, taken from \citet{hastie_elements_2009}.



