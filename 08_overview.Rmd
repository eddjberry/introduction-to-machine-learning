# Process Overview

<span class="newthought">Despite the facade of a polished product</span> one finds in published research, most of the approach with the statistical analysis of data is full of data preparation, starts and stops, debugging, re-analysis, tweaking and fine-tuning etc. Statistical learning is no different in this sense.  Before we begin with explicit examples, it might be best to give a general overview of the path we'll take.


## Data Preparation

As with any typical statistical project, probably most of the time will be spent preparing the data for analysis.  Data is never ready to analyze right away, and careful checks must be made in order to ensure the integrity of the information.  This would include correcting errors of entry, noting extreme values, possibly imputing missing data and so forth.  In addition to these typical activities, we will discuss a couple more things to think about during this initial data examination when engaged in machine learning.


### Define Data and Data Partitions

As we have noted previously, ideally we will have enough data to create a hold-out, test, or validation data set. This would be some random partition of the data such that we could safely conclude that the data in the test set comes from the same population as the training set. The training set is used to fit the initial models at various tuning parameter settings, with a 'best' model being that which satisfies some criterion on the validation set (or via a general validation process).  With final model and parameters chosen, generalization error will be assessed with the the performance of the final model on the test data.


### Feature Scaling

Even with standard regression modeling, centering continuous variables (subtracting the mean) is a good idea so that intercepts and zero points in general are meaningful. Standardizing variables so that they have similar variances or ranges will help some procedures find their minimums faster.  Another common transformation is *min-max* normalization[^minmax], which will transfer a scale to a new one of some chosen minimum and maximum.  Note that whatever approach is done, it must be done *after* any explicit separation of data.  So if you have separate training and test sets, they should be scaled separately.


### Feature Engineering

If we're lucky we'll have ideas on potential combinations or other transformations of the predictors we have available.  For example, in typical social science research there are two-way interactions one is often predisposed to try, or perhaps one can sum multiple items to a single scale score that may be more relevant.  Another common technique is to use a dimension reduction scheme such as principal components, but this can (and probably should) actually be an implemented algorithm in the ML process[^pcr].

One can implement a variety of such approaches in ML as well to create additional potentially relevant features, even automatically, but as a reminder, a key concern is overfitting, and doing broad construction of this sort with no contextual guidance would potentially be prone to such a pitfall.  In other cases it may simply be not worth the time expense.


### Discretization

While there may be some contextual exceptions to the rule, it is generally a pretty bad idea in standard statistical modeling to discretize/categorize continuous variables[^discretize2].  However some ML procedures will work better (or just faster) if dealing with discrete valued predictors rather than continuous. Others even require them; for example, logic regression needs binary input.  While one could pick arbitrary intervals and cutpoints in an unsupervised fashion such as picking equal range bins or equal frequency bins, there are supervised algorithmic approaches that will use the information in the data to produce some 'optimal' discretization.

It's generally not a good idea to force things in data analysis, and given that a lot of data situations will be highly mixed, it seems easier to simply apply some scaling to preserve the inherent relationships in the data.  Again though, if one has only a relative few continuous variables or a context in which it makes sense to, it's probably better to leave continuous variables as such.


## Model Selection

With data prepared and ready to analyze, one can use a validation process to come up with a viable model.  Use an optimization procedure or a simple grid search over a set of specific values to examine models at different tuning parameters. Perhaps make a finer search once an initial range of good performing values is found, though one should not split hairs over arbitrarily close performance.  Select a 'best' model given some criterion such as overall accuracy, or if concerned about over fitting, select the simplest model within one standard error of the accuracy of the best, or perhaps the simplest within X% of the best model.  For highly skewed classes, one might need to use a different measure of performance besides accuracy.  If one has a great many predictor variables, one may use the model selection process to select features that are 'most important'.


## Model Assessment

With tuning parameters/features chosen, we then examine performance on the independent test set (or via some validation procedure). For classification problems, consider other statistics besides accuracy as measures of performance, especially if classes are unbalanced. Consider other analytical techniques that are applicable and compare performance among the different approaches.  One can even combine disparate models' predictions to possibly create an even better classifier[^ensemble].

[^minmax]: See, for example, the <span class="func">rescale</span> function in the <span class="pack">scales</span> package.

[^pcr]: For example, via principal components or partial least squares regression.

[^discretize2]: See @harrell2015regression for a good summary of reasons why not to.

[^ensemble]: The topic of ensembles is briefly noted later.