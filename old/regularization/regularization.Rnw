% !Rnw root = ../mlcrash.Rnw

<<regsetup, cache=FALSE, include=FALSE>>=
  opts_knit$set(self.contained=FALSE)
@


%%%%%%%%%%%%%%%%%%%%%
\part{Regularization}
%%%%%%%%%%%%%%%%%%%%%
\newthought{It is important to note} that a model fit to a single data set might do very well with the data at hand, but then suffer when predicting independent data \sidenote{In terminology we will discuss further later, such models might have low bias but notable variance.}.  Also, oftentimes we are interested in a 'best' subset of predictors among a great many, and in this scenario the estimated coefficients are overly optimistic.  This general issue can be improved by shrinking estimates toward zero, such that some of the performance in the initial fit is sacrificed for improvement with regard to prediction.

Penalized estimation will provide estimates with some shrinkage, and we can use it with little additional effort with our common procedures.  Concretely, let's apply this to the standard linear model, where we are finding estimates of $\beta$ that minimize the squared error loss.

\vspace{.25cm}
$\hat\beta = \underset{\beta}{\mathrm{arg\, min}} \sum{(y-X\beta)^2}$ 
\vspace{.25cm}

In words, we're finding the coefficients that minimize the sum of the squared residuals.  With the approach to regression here we just add a penalty component to the procedure as follows.

\vspace{.25cm}
$\hat\beta = \underset{\beta}{\mathrm{arg\, min}} \sum{(y-X\beta)^2} + \lambda\overset{p}{\underset{j=1}{\sum}}{\left|\beta_j\right|}$ 
\vspace{.25cm}

%
In the above equation, $\lambda$ is our penalty term\sidenote{This can be set explicitly or also estimated via a validation approach.  As we do not know it beforehand, we can estimate it on a validation data set (not the test set) and then use the estimated value when estimating coefficients via cross-validation with the test set.  We will talk more about validation later.} for which larger values will result in more shrinkage.  It's applied to the $L_1$ or Manhattan norm of the coefficients, $\beta_1,\beta_2...\beta_p$, i.e. \emph{not including the intercept} $\beta_0$, and is the sum of their absolute values (commonly referred to as the \emph{lasso}\sidenote{See Tibshirani (1996) Regression shrinkage and selection via the lasso.}). For generalized linear and additive models, we can conceptually express a penalized likelihood as follows:

\vspace{.25cm}
$l_p(\beta) = l(\beta) - \lambda\overset{p}{\underset{j=1}{\sum}}{\left|\beta_j\right|}$ 
\vspace{.25cm}

As we are maximizing the likelihood the penalty is a subtraction, but nothing inherently different is shown.  This basic idea of adding a penalty term will be applied to all machine learning approaches, but as shown, we can apply such a tool to classical methods to boost prediction performance.

It should be noted that we can go about the regularization in different ways.  For example, using the squared $L_2$ norm results in what is called \emph{ridge} regression (a.k.a. Tikhonov regularization)\sidenote{Interestingly, the lasso and ridge regression results can be seen as a Bayesian approach using a zero mean Laplace and Normal prior distribution respectively for the $\beta_j$.}, and using a weighted combination of the lasso and ridge penalties gives us \emph{elastic net} regularization. 

%%%
\section{R Example}
In the following example, we take a look at the lasso approach for a standard linear model.  We add the regularization component, with a fixed penalty $\lambda$ for demonstration purposes\sidenote{As noted previously, in practice $\lambda$ would be estimated via some validation procedure.}.  However you should insert your own values for $\lambda$ in the \texttt{\textcolor{red}{optim}} line to see how the results are affected. 

\vspace{.25cm}
<<regularization, cache=TRUE, warning=F, eval=TRUE, tidy=F, dependson=-1, size="footnotesize">>=
sqerrloss_reg = function(beta, X, y, lambda=.1){
  mu = X%*%beta
  sum((y-mu)^2) + lambda*sum(abs(beta[-1]))
}

out3 = optim(par=c(0,0,0), fn=sqerrloss_reg, X=X, y=y)
rbind(c(out1$par, out1$value), 
      c(coef(out2),sum(resid(out2)^2)), 
      c(out3$par, out3$value) )
@

From the above, we can see in this case that the predictor coefficients have indeed shrunk toward zero slightly while the residual sum of squares has increased just a tad.

In general, we can add the same sort of penalty to any number of models, such as logistic regression, neural net models, recommender systems etc.  The primary goal again is to hopefully increase our ability to generalize the selected model to new data.  Note that the estimates produced are in fact biased, but we have decreased the variance with new predictions as a counterbalance, and this brings us to the topic of the next section.
