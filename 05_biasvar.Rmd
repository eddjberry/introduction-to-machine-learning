# Bias-Variance Tradeoff

<span class="newthought">In most of science,</span> we are concerned with reducing uncertainty in our knowledge of some phenomenon.  The more we know about the factors involved or related to some outcome of interest, the better we can predict that outcome upon the influx of new information.  The initial step is to take the data at hand, and determine how well a model or set of models fit the data in various fashions.  In many applications however, this part is also more or less the end of the game as well[^bigdata].

Unfortunately, such an approach in which we only fit models to one data set does not give a very good sense of <span class="emph">generalization</span> performance, i.e. the performance we would see with new data.  While typically not reported, most researchers, if they are spending appropriate time with the data, are actually testing a great many models, for which the 'best' is then provided in detail in the end report.  Without some generalization performance check however, such performance is overstated when it comes to new data.

In the following consider a standard linear model scenario, e.g. with squared-error loss function and perhaps some regularization, and a data set in which we split the data in some random fashion into a <span class='emph'>training set</span> , for initial model fit, and a<span class="emph">test set</span>, which is a separate and independent data set, to measure generalization performance[^tuning].  We note <span class="emph">training error</span> as the (average) loss over the training set, and <span class="emph">test error</span> as the (average) prediction error obtained when a model resulting from the training data is fit to the test data.  So in addition to the previously noted goal of finding the 'best' model (<span class="emph">model selection</span>), we are interested further in estimating the prediction error with new data (<span class="emph">model performance</span>).

## Bias *&* Variance

Conceptually<span class="marginnote">Much of the following is essentially a paraphrase of parts of @hastie_elements_2009 (chapters 2 and 7).</span>, with the standard model $Y = f(X) + \epsilon$ with we can think of the expected prediction error at a specific input $X = x_0$ as: 


$$\text{Error}_{x_0} = \text{Irreducible Error} + \text{Bias}^2 + \text{Variance}$$


To better understand this, think of training models over and over, each time with new training data, but testing each model at $x_0$. The $\text{Error}_{x_0}$ is the average, or expected value of the MSE in this scenario, or $E(y - \hat f(x))^2$. In other words, we have three components to our general notion of prediction error: 

- Irreducible error: The variance of the (new test) target ($\sigma^2_\epsilon$). This is unavoidable, since our $y$ is measured with error..
- $Bias^2$: the amount the *average* of our estimate varies from the true mean ($E(\hat f) - f$). This is typically the result of trying to model the complexity of nature with something much simple that the human brain can understand.
- Variance: the amount by which our prediction would change if we estimated it using a different training data set ($Var(\hat f)$)

Slightly more formally, we can present this as follows, with $h_0$ our estimated (hypothesized) value at $x_0$:


$$\text{Error}_{x_0} = Var(\epsilon) + (\text{E}[h_0] - f(x_0))^2 + Var(h_0)$$

The latter two make up the mean squared error in our previous demonstration.  While they are under our control, they compete with one another such that oftentimes we improve one at the detriment of the other.



## The Tradeoff

Outlining a general procedure, we start by noting the prediction error on a training data set with multiple models of varying complexity (e.g. increasing the number of predictor variables, adding polynomial terms, including interactions), and then assess the performance of the chosen models in terms of prediction error on the test set.  We then perform the same activity for a total of 100 simulated data sets, for each level of complexity.

<span class="marginnote"><img src="img/biasvar2.svg" style="display:block; margin: 0 auto;" width=75%></span>The results from this process might look like the image to the right taken from @hastie_elements_2009.  With regard to the training data, we have $\mathrm{error}_{train}$ for one hundred training sets for each level of model complexity.  The bold blue line notes this average error over the 100 sets by model complexity.  The bold red line the average test error ($\mathrm{error}_{test}$) across the 100 test data sets.

Ideally we'd like to see low bias and variance, but things are not so easy. One thing we can see clearly is that $\mathrm{error}_{train}$ is not a good estimate of $\mathrm{error}_{test}$, which is now our focus in terms of performance. If we think of the training error as what we would see in typical research where one does everything with a single data set, we are using the same data set to fit the model and assess error.  As the model is adapted to that data set specifically, it will be overly optimistic in the estimate of the error, that optimism being the difference between the error rate we see based on the training data versus the average of what we would get with many test data sets.  We can think of this as a problem of overfitting to the training data.  Models that do not incorporate any regularization or validation process of any kind are likely overfit to the data presented.

Generally speaking, the more complex the model, the lower the bias, but the higher the variance, as depicted in the graphic.  Specifically however, the situation is more nuanced, where type of problem (classification with 0-1 loss vs. continuous response with squared error loss[^biasvardimen]) and technique (a standard linear model vs. regularized fit) will exhibit different bias-variance relationships.


<span class="marginnote"><img src="img/biasvartarget.svg" style="display:block; margin: 0 auto;"> <br> Figure adapted from @domingos_few_2012.</span>

## Diagnosing Bias-Variance Issues *&* Possible Solutions

Let's assume a regularized linear model with a standard data split into training and test sets.  We will describe different scenarios with possible solutions.


### Worst Case Scenario

Starting with the worst case scenario, poor models may exhibit high bias and high variance.  One thing that will not help this situation (perhaps contrary to intuition) is adding more data, i.e. increasing N.  You can't make a silk purse out of a sow's ear ([*usually*](https://libraries.mit.edu/archives/exhibits/purse/)), and adding more data just gives you a more accurate picture of how awful your model is. One might need to rework the model, e.g. adding new predictors or creating them via interaction terms, polynomials, or other smooth functions as in additive models, or simply collecting better and/or more relevant data.<span class="marginnote"><img src="img/biasvar_gp.svg" style="display:block; margin: 0 auto;"> <br> Figure inspired by @murphy_machine_2012 (figure 6.5) showing the bias-variance tradeoff.  Sample (left) and average (right) fits of linear regression using a Gaussian radial basis function expansion. The green line represents the true relationship. The top row shows low variance between one fit and the next (left) but notable bias (right) in that the average fit is off.  Compare to the less regularized (high variance, low bias) situation of the bottom row.  See the <span class="pack">kernlab</span> package for the fitting function used.</span>


### High Variance

When variance is a problem, our training error is low while test error is relatively high (overfitting problem). Implementing more shrinkage or other penalization to model complexity may help with the issue.  In this case more data may help as well.

### High Bias

With bias issues, our training error is high and test error is not too different from training error (underfitting problem).  Adding new predictors/features, e.g. interaction terms, polynomials etc., can help here.  Additionally reducing the penalty parameter $\lambda$ would also work with even less effort, though generally it should be estimated rather than explicitly set.


[^bigdata]: I should note that I do not make any particular claim about the quality of such analysis.  In many situations the cost of data collection is very high, and for all the current enamorment with 'big' data, a lot of folks will never have access to big data for their situation (e.g. certain clinical populations).  In these situations getting new data for which one might make predictions is extremely difficult.

[^tuning]: In typical situations there are parameters specific to some analytical technique for which one would have no knowledge, and which must be estimated along with the usual parameters of the standard models. The $\lambda$ penalty parameter in regularized regression is one example of such a <span class='emph'>tuning parameter</span>. In the best case scenario, we would also have a <span class='emph'>validation set</span>, where we could determine appropriate values for such parameters based on performance with the validation data set, and then assess generalization performance on the test set when the final model has been chosen.  However, methods are available to us in which we can approximate the validation step in other ways.

[^biasvardimen]: See Friedman (1996) *On Bias, Variance, 0/1 Loss and the Curse of Dimensionality* for the unusual situations that can arise in dealing with classification error with regard to bias and variance.