% !Rnw root = ../mlcrash.Rnw

<<biasvar, cache=FALSE, include=FALSE>>=
  opts_knit$set(self.contained=FALSE)
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\part{Bias-Variance Tradeoff}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newthought{In most of science} we are concerned with reducing uncertainty in our knowledge of some phenomenon.  The more we know about the factors involved or related to some outcome of interest, the better we can predict that outcome upon the influx of new information.  The initial step is to take the data at hand, and determine how well a model or set of models fit the data in various fashions.  In many applications however, this part is also more or less the end of the game as well\sidenote{I should note that I do not make any particular claim about the quality of such analysis.  In many situations the cost of data collection is very high, and for all the current enamorment with 'big' data, a lot of folks will never have access to big data for their situation (e.g. certain clinical populations).  In these situations getting new data for which one might make predictions is extremely difficult.}.

Unfortunately, such an approach in which we only fit models to one data set does not give a very good sense of \emph{generalization} performance, i.e. the performance we would see with new data.  While typically not reported, most researchers, if they are spending appropriate time with the data, are actually testing a great many models, for which the 'best' is then provided in detail in the end report.  Without some generalization performance check however, such performance is overstated when it comes to new data.

In the following consider a standard linear model scenario, e.g. with squared-error loss function and perhaps some regularization, and a data set in which we split the data in some random fashion into a \emph{training} set, for initial model fit, and a \emph{test} set, which is a separate and independent data set, to measure generalization performance\sidenote{In typical situations there are parameters specific to some analytical technique for which one would have no knowledge and which must be estimated along with the usual parameters of the standard models. The $\lambda$ penalty parameter in regularized regression is one example of such a \emph{tuning} parameter. In the best case scenario, we would also have a \emph{validation} set, where we could determine appropriate values for such parameters based on performance with the validation data set, and then assess generalization performance on the test set when the final model has been chosen.  However, methods are available to us in which we can approximate the validation step in other ways.}.  We note \emph{training error} as the (average) loss over the training set, and \emph{test error} as the (average) prediction error obtained when a model resulting from the training data is fit to the test data.  So in addition to the previously noted goal of finding the 'best' model (\emph{model selection}), we are interested further in estimating the prediction error with new data (\emph{model performance}).

%%%
\section{Bias \& Variance}
Conceptually\sidenote{Much of the following is essentially a paraphrase of parts of \citet[Chap. 2 \& 7]{hastie_elements_2009}.}, with the standard model $Y = f(X) + \epsilon$ with we can think of the expected prediction error at a specific input $X = x_0$ as: \\

\vspace{.25cm}
\noindent $\text{Error}_{x_0} = \text{Irreducible Error} + \text{Bias}^2 + \text{Variance}$
\vspace{.25cm}

In other words, we have three components to our general notion of prediction error: \\

\begin{description}
  \item[$\sigma^2_\epsilon$] An initial variance of the target around the true mean $f(x_0)$ (unavoidable).
  \item[$Bias^2$] the amount the \emph{average} of our estimate varies from the true mean.
  \item[Variance] the variance of the target value about its mean.
\end{description}

Slightly more formally, we can present this as follows, with $h_0$ our estimated (hypothesized) value:

\vspace{.25cm}
\noindent $\text{Error}_{x_0} = Var(\epsilon) + (\text{E}[h_0] - f(x_0))^2 + Var(h_0) $
\vspace{.25cm}


%%%
\section{The Tradeoff}
Outlining a general procedure, we start by noting the prediction error on a training data set with multiple models of varying complexity (e.g. increasing the number of predictor variables), and then assess the performance of the chosen models in terms of prediction error on the test set.  We then perform the same activity for a total of 100 simulated data sets, for each level of complexity.

\marginnote{\includegraphics[scale=.5]{images/biasvar2}}The results from this process might look like the image to the right taken from \citet{hastie_elements_2009}.  With regard to the training data, we have $error_{train}$ for one hundred training sets for each level of model complexity.  The bold blue line notes this average error over the 100 sets by model complexity.  The bold red line the average test error ($error_{test}$) across the 100 test data sets.

Ideally we'd like to see low bias and variance, but things are not so easy. One thing we can see clearly is that $error_{train}$ is not a good estimate of $error_{test}$, which is now our focus in terms of performance. If we think of the training error as what we would see in typical research where one does everything with a single data set, we are using the same data set to fit the model and assess error.  As the model is adapted to that data set specifically, it will be overly optimistic in the estimate of the error, that optimism being the difference between the error rate we see based on the training data versus the average of what we would get with many test data sets.  We can think of this as a problem of overfitting to the training data.  Models that do not incorporate any regularization or validation process of any kind are likely overfit to the data presented.

Generally speaking, the more complex the model, the lower the bias, but the higher the variance, as depicted in the graphic.  Specifically however, the situation is more nuanced, where type of problem (classification with 0-1 loss vs. continuous response with squared error loss\sidenote{See Friedman (1996) \emph{On Bias, Variance, 0/1 Loss and the Curse of Dimensionality} for the unusal situations that can arise in dealing with classification error with regard to bias and variance.}) and technique (a standard linear model vs. regularized fit) will exhibit different bias-variance relationships.

%%%
\section{Diagnosing Bias-Variance Issues \& Possible Solutions}
Let's assume a regularized linear model with a standard data split into training and test sets.  We will describe different scenarios with possible solutions.

\marginnote{\includegraphics[scale=.35]{images/biasvartarget} \footnotesize{Figure adapted from \citet{domingos_few_2012}.}}
\subsection{Worst Case Scenario}
Starting with the worst case scenario, poor models may exhibit high bias and high variance.  One thing that will not help this situation (perhaps contrary to intuition) is adding more data, i.e. increasing N.  You can't make a silk purse out of a sow's ear (\emph{usually}\sidenote{\url{https://libraries.mit.edu/archives/exhibits/purse/}}), and adding more data just gives you a more accurate picture of how awful your model is. One might need to rework the model, e.g. adding new predictors or creating them via interaction terms, polynomials, or other smooth functions as in additive models, or simply collecting better and/or more relevant data.\marginnote{\includegraphics[scale=.35]{images/graph64} \footnotesize{Figure inspired by \citet[figure 6.5]{murphy_machine_2012} showing the bias-variance tradeoff.  Sample (left) and average (right) fits of linear regression using a gaussian radial basis function expansion. The green line represents the true relationship. The top row shows low variance between one fit and the next (left) but notable bias (right) in that the average fit is off.  Compare to the less regularized (high variance, low bias) situation of the bottom row.  See the \emph{\textcolor{blue}{kernlab}} package for the fitting function used.}}


\subsection{High Variance}
When variance is a problem, our training error is low while test error is relatively high (overfitting problem). Implementing more shrinkage or other penalization to model complexity may help with the issue.  In this case more data may help as well.

\subsection{High Bias}
With bias issues our training error is high and test error is not too different from training error (underfitting problem).  Adding new predictors/features, interaction terms, polynomials etc. can help here.  Additionally reducing the penalty parameter $\lambda$ would also work with even less effort, though generally it should be estimated rather than explicitly set.
