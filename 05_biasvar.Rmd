# Bias-Variance Tradeoff

<span class="newthought">In most of science,</span> we are concerned with reducing uncertainty in our knowledge of some phenomenon.  The more we know about the factors involved or related to some outcome of interest, the better we can predict that outcome upon the influx of new information.  The initial step is to take the data at hand, and determine how well a model or set of models fit the data in various fashions.  In many applications however, this part is also more or less the end of the game as well[^bigdata].

Unfortunately, such an approach in which we only fit models to one data set does not give a very good sense of <span class="emph">generalization</span> performance, i.e. the performance we would see with new data.  While typically not reported, most researchers, if they are spending appropriate time with the data, are actually testing a great many models, for which the 'best' is then provided in detail in the end report.  Without some generalization performance check however, such performance is overstated when it comes to new data.

In the following consider a standard linear model scenario, e.g. with squared-error loss function and perhaps some regularization, and a data set in which we split the data in some random fashion into a <span class='emph'>training set</span> , for initial model fit, and a<span class="emph">test set</span>, which is a separate and independent data set, to measure generalization performance[^tuning].  We note <span class="emph">training error</span> as the (average) loss over the training set, and <span class="emph">test error</span> as the (average) prediction error obtained when a model resulting from the training data is fit to the test data.  So in addition to the previously noted goal of finding the 'best' model (<span class="emph">model selection</span>), we are interested further in estimating the prediction error with new data (<span class="emph">model performance</span>).

## Bias *&* Variance

<span class="marginnote">Much of the following is essentially a paraphrase of parts of @hastie_elements_2009 (chapters 2 and 7).</span>We start with a true data generating process for some target $y$, expressed as a function of features $X$. We can specify the true model as 

$$y = f(X) + \epsilon$$

where $f(x)$ is the expected value of $y$ given $X$, i.e. $f(x) = E(y|X)$, and the expected value of the error 0, $E(\epsilon)=0$, with some variance, $\textrm{Var}(\epsilon) = \sigma^2_\epsilon$.  In other words, we are talking about the standard regression model we all know and love.  Now we can conceptually think of the *expected prediction error* at a specific input $X = x_0$ as: 


$$\text{Error}_{x_0} = \text{Irreducible Error} + \text{Bias}^2 + \text{Variance}$$


To better understand this, think of training models over and over, each time with new training data, but testing each model at input $x_0$. The $\text{Error}_{x_0}$ is the average, or expected value of the prediction error in this scenario, or $E[(y - \hat f(x))^2|X=x_0]$, with $\hat f$ our current estimate of the true underlying data generating function $f$. In other words, we have three components to our general notion of prediction error: 

**Irreducible error**: The variance of the (new test) target ($\sigma^2_\epsilon$). This is unavoidable, since our $y$ is measured with error.

**$\textrm{Bias}^2$**: the amount the *average* of our estimate varies from the true (but unknown) value ($E(\hat f) - f$). This is typically the result of trying to model the complexity of nature with something much simpler that the human brain can understand. While the simpler might make us feel good, it may not work very well.

**Variance**: the amount by which our prediction would change if we had estimated it using a different training data set ($Var(\hat f)$). Even with unbiased estimates, we could still see a high mean squared error due to high variance.

Slightly more formally, we can present this as follows, with $h_0$ our estimated (hypothesized) value at $x_0$:


$$\text{Error}_{x_0} = Var(\epsilon) + (\text{E}[h_0] - f(x_0))^2 + Var(h_0)$$

The latter two components make up the mean squared error in our previous demonstration.  While they are under our control, they compete with one another such that oftentimes we improve one at the detriment of the other. In other words, *bias and variance are not independent*.



## The Tradeoff

Outlining a general procedure, we start by noting the prediction error on a training data set with multiple models of varying complexity (e.g. increasing the number of predictor variables, adding polynomial terms, including interactions), and then assess the performance of the chosen models in terms of prediction error on the test set.  We then perform the same activity for a total of 100 simulated data sets, for each level of complexity.

<span class="marginnote"><img src="img/biasvar2.svg" style="display:block; margin: 0 auto;" width=100%></span>The results from this process might look like the image to the right taken from @hastie_elements_2009.  With regard to the training data, we have $\mathrm{error}_{\mathrm{train}}$ for one hundred training sets for each level of model complexity.  The bold blue line notes this average error over the 100 sets by model complexity, and we can see that more complex models fit the data better.  The bold red line the average test error ($\mathrm{error}_{\mathrm{test}}$) across the 100 test data sets, and it tells a differen story.

Ideally we'd like to see low bias and (relatively) low  variance, but things are not so easy. One thing we can see clearly is that $\mathrm{error}_{\mathrm{train}}$ is not a good estimate of $\mathrm{error}_{\mathrm{test}}$, which is now our focus in terms of performance. If we think of the training error as what we would see in typical research where one does everything with a single data set, we are using the same data set to fit the model and assess error.  As the model is adapted to that data set specifically, it will be overly optimistic in the estimate of the error, that optimism being the difference between the error rate we see based on the training data versus the average of what we would get with many test data sets.  We can think of this as a problem of overfitting to the training data.  Models that do not incorporate any regularization or validation process of any kind are likely overfit to the data presented.

Generally speaking, the more complex the model, the lower the bias, but the higher the variance, as depicted in the graphic.  Specifically however, the situation is more nuanced, where the type of problem (classification with 0-1 loss vs. continuous response with squared error loss[^biasvardimen]) and technique (a standard linear model vs. regularized fit) will exhibit different bias-variance relationships.


<span class="marginnote"><img src="img/biasvartarget.svg" style="display:block; margin: 0 auto;"> <br> Figure adapted from @domingos_few_2012.</span>

## Diagnosing Bias-Variance Issues *&* Possible Solutions

Let's assume a regularized linear model with a standard data split into training and test sets.  We will describe different scenarios with possible solutions.


### Worst Case Scenario

Starting with the worst case scenario, poor models may exhibit high bias and high variance.  One thing that will not help this situation (perhaps contrary to intuition) is adding more data.  You can't make a silk purse out of a sow's ear ([*usually*](https://libraries.mit.edu/archives/exhibits/purse/)), and adding more data just gives you a more accurate picture of how awful your model is. One might need to rework the model, e.g. adding new predictors or creating them via interaction terms, polynomials, or other smooth functions as in additive models, or simply collecting better and/or more relevant data.<span class="marginnote"><img class='imgbigger' src="img/biasvar_gp.svg" style="display:block; margin: 0 auto;"> <br> Figure inspired by @murphy_machine_2012 (figure 6.5) showing the bias-variance tradeoff.  Sample (left) and average (right) fits of linear regression using a Gaussian radial basis function expansion. The green line represents the true relationship. The top row shows low variance between one fit and the next (left) but notable bias (right) in that the average fit is off.  Compare to the less regularized (high variance, low bias) situation of the bottom row.  See the <span class="pack">kernlab</span> package for the fitting function used, and the [appendix][Appendix] for the code used to produce the graph.</span>


### High Variance

When variance is a problem, our training error is low while test error is relatively high (overfitting problem). Implementing more shrinkage or other penalization to model complexity may help with the issue.  In this case more data may help as well.

### High Bias

With bias issues, our training error is high and test error is not too different from training error (underfitting problem).  Adding new predictors/features, e.g. interaction terms, polynomials etc., can help here.  Additionally reducing the penalty parameter $\lambda$ would also work with even less effort, though generally it should be estimated rather than explicitly set.


## Bias-Variance Summary

One of key ideas any applied researcher can take from machine learning concerns the bias-variance tradeoff and issues of overfitting in particular.  Typical applied practice involves potentially dozens of models fit to the same data set without any validation whatsoever, yet only one or two are actually presented in publication.  Many disciplines report nothing but the statistical significance, and yet one can have statistically significant predictors and have predictive capability that is no different from guessing.  Furthermore very complex models are often fit to small data sets, compounding the problem.

It is very easy to describe ***science*** without ever talking about statistical significance.  It is impossible to talk about science without talking about prediction.  The bias-variance tradeoff is one way to bring the concerns of prediction to the forefront, and any applied researcher can benefit from thinking about its implications. 



[^bigdata]: I should note that I do not make any particular claim about the quality of such analysis.  In many situations the cost of data collection is very high, and for all the current enamorment with 'big' data, a lot of folks will never have access to big data for their situation (e.g. certain clinical populations).  In these situations getting new data for which one might make predictions is extremely difficult.

[^tuning]: In typical situations there are parameters specific to some analytical technique for which one would have no knowledge, and which must be estimated along with the usual parameters of the standard models. The $\lambda$ penalty parameter in regularized regression is one example of such a <span class='emph'>tuning parameter</span>. In the best case scenario, we would also have a <span class='emph'>validation set</span>, where we could determine appropriate values for such parameters based on performance with the validation data set, and then assess generalization performance on the test set when the final model has been chosen.  However, methods are available to us in which we can approximate the validation step in other ways.

[^biasvardimen]: See Friedman (1996) *On Bias, Variance, 0/1 Loss and the Curse of Dimensionality* for the unusual situations that can arise in dealing with classification error with regard to bias and variance.